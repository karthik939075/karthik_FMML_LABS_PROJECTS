{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthik939075/karthik_FMML_LABS_PROJECTS/blob/main/FMML_M1L2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine Learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2\n",
        "\n",
        "In this lab, we will show a part of the ML pipeline by using the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district. We will use the scikit-learn library to load the data and perform some basic data preprocessing and model training. We will also show how to evaluate the model using some common metrics, split the data into training and testing sets, and use cross-validation to get a better estimate of the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = np.random.default_rng(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LpqjN991GGJ",
        "outputId": "f0a30b68-ccfe-41c8-82d5-1dec1d6ad0b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _california_housing_dataset:\n",
            "\n",
            "California Housing dataset\n",
            "--------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 20640\n",
            "\n",
            "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
            "\n",
            "    :Attribute Information:\n",
            "        - MedInc        median income in block group\n",
            "        - HouseAge      median house age in block group\n",
            "        - AveRooms      average number of rooms per household\n",
            "        - AveBedrms     average number of bedrooms per household\n",
            "        - Population    block group population\n",
            "        - AveOccup      average number of household members\n",
            "        - Latitude      block group latitude\n",
            "        - Longitude     block group longitude\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "This dataset was obtained from the StatLib repository.\n",
            "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
            "\n",
            "The target variable is the median house value for California districts,\n",
            "expressed in hundreds of thousands of dollars ($100,000).\n",
            "\n",
            "This dataset was derived from the 1990 U.S. census, using one row per census\n",
            "block group. A block group is the smallest geographical unit for which the U.S.\n",
            "Census Bureau publishes sample data (a block group typically has a population\n",
            "of 600 to 3,000 people).\n",
            "\n",
            "A household is a group of people residing within a home. Since the average\n",
            "number of rooms and bedrooms in this dataset are provided per household, these\n",
            "columns may take surprisingly large values for block groups with few households\n",
            "and many empty houses, such as vacation resorts.\n",
            "\n",
            "It can be downloaded/loaded using the\n",
            ":func:`sklearn.datasets.fetch_california_housing` function.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
            "      Statistics and Probability Letters, 33 (1997) 291-297\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = datasets.fetch_california_housing()\n",
        "# Dataset description\n",
        "print(dataset.DESCR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCe1VNftevgE"
      },
      "source": [
        "Given below are the list of target values. These correspond to the house value derived considering all the 8 input features and are continuous values. We should use regression models to predict these values but we will start with a simple classification model for the sake of simplicity. We need to just round off the values to the nearest integer and use a classification model to predict the house value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8K0ggBOevgE",
        "outputId": "183fd3e1-a3ad-4b7e-d6ca-52d9efe5d57f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orignal target values: [4.526 3.585 3.521 ... 0.923 0.847 0.894]\n",
            "Target values after conversion: [4 3 3 ... 0 0 0]\n",
            "Input variables shape: (20640, 8)\n",
            "Output variables shape: (20640,)\n"
          ]
        }
      ],
      "source": [
        "print(\"Orignal target values:\", dataset.target)\n",
        "\n",
        "dataset.target = dataset.target.astype(int)\n",
        "\n",
        "print(\"Target values after conversion:\", dataset.target)\n",
        "print(\"Input variables shape:\", dataset.data.shape)\n",
        "print(\"Output variables shape:\", dataset.target.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "The simplest model to use for classification is the K-Nearest Neighbors model. We will use this model to predict the house value with a K value of 1. We will also use the accuracy metric to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "outputs": [],
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and a query point\n",
        "    and returns the predicted label for the query point using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    query: numpy array of shape (d,) where d is the number of features\n",
        "\n",
        "    returns: the predicted label for the query point which is the label of the training data which is closest to the query point\n",
        "    \"\"\"\n",
        "    diff = (\n",
        "        traindata - query\n",
        "    )  # find the difference between features. Numpy automatically takes care of the size here\n",
        "    sq = diff * diff  # square the differences\n",
        "    dist = sq.sum(1)  # add up the squares\n",
        "    label = trainlabel[np.argmin(dist)]\n",
        "    return label\n",
        "\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and test data\n",
        "    and returns the predicted labels for the test data using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    testdata: numpy array of shape (m,d) where m is the number of test samples and d is the number of features\n",
        "\n",
        "    returns: the predicted labels for the test data which is the label of the training data which is closest to each test point\n",
        "    \"\"\"\n",
        "    predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "    return predlabel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "outputs": [],
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and test data\n",
        "    and returns the predicted labels for the test data using the random classifier algorithm\n",
        "\n",
        "    In reality, we don't need these arguments but we are passing them to keep the function signature consistent with other classifiers\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    testdata: numpy array of shape (m,d) where m is the number of test samples and d is the number of features\n",
        "\n",
        "    returns: the predicted labels for the test data which is a random label from the training data\n",
        "    \"\"\"\n",
        "\n",
        "    classes = np.unique(trainlabel)\n",
        "    rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "    predlabel = classes[rints]\n",
        "    return predlabel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "We need a metric to evaluate the performance of the model. Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm. We will use the accuracy metric to evaluate and compate the performance of the K-Nearest Neighbors model and the random classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "outputs": [],
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "    \"\"\"\n",
        "    This function takes in the ground-truth labels and predicted labels\n",
        "    and returns the accuracy of the classifier\n",
        "\n",
        "    gtlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    predlabel: numpy array of shape (n,) where n is the number of samples\n",
        "\n",
        "    returns: the accuracy of the classifier which is the number of correct predictions divided by the total number of predictions\n",
        "    \"\"\"\n",
        "    assert len(gtlabel) == len(\n",
        "        predlabel\n",
        "    ), \"Length of the ground-truth labels and predicted labels should be the same\"\n",
        "    correct = (\n",
        "        gtlabel == predlabel\n",
        "    ).sum()  # count the number of times the groundtruth label is equal to the predicted label.\n",
        "    return correct / len(gtlabel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability. We will use this function to split the dataset into training and testing sets. We will use the training set to train the model and the testing set to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "outputs": [],
      "source": [
        "def split(data, label, percent):\n",
        "    # generate a random number for each sample\n",
        "    rnd = rng.random(len(label))\n",
        "    split1 = rnd < percent\n",
        "    split2 = rnd >= percent\n",
        "\n",
        "    split1data = data[split1, :]\n",
        "    split1label = label[split1]\n",
        "    split2data = data[split2, :]\n",
        "    split2label = label[split2]\n",
        "    return split1data, split1label, split2data, split2label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBZkHBLJ1iU-",
        "outputId": "73868e7d-9aca-4e01-b8ab-e327a8ead37e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples: 4144\n",
            "Number of train samples: 16496\n",
            "Percent of test data: 20.07751937984496 %\n"
          ]
        }
      ],
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(\n",
        "    dataset.data, dataset.target, 20 / 100\n",
        ")\n",
        "print(\"Number of test samples:\", len(testlabel))\n",
        "print(\"Number of train samples:\", len(alltrainlabel))\n",
        "print(\"Percent of test data:\", len(testlabel) * 100 / len(dataset.target), \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "outputs": [],
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(\n",
        "    alltraindata, alltrainlabel, 75 / 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBlZDTHUFTZx",
        "outputId": "f91ba59a-5a20-4448-ee74-320028374ad3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy using nearest neighbour algorithm: 100.0 %\n",
            "Training accuracy using random classifier:  16.4375808538163 %\n"
          ]
        }
      ],
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Training accuracy using nearest neighbour algorithm:\", trainAccuracy*100, \"%\")\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Training accuracy using random classifier: \", trainAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case. This is because the random classifier randomly assigns a label to each sample and the probability of assigning the correct label is 1/(number of classes). Let us predict the labels for our validation set and get the accuracy. This accuracy is a good estimate of the accuracy of our model on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h7bXoW_2H3v",
        "outputId": "f1af718b-a4ef-4d8c-be20-7bf88ad55d7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour algorithm: 34.10852713178294 %\n",
            "Validation accuracy using random classifier: 16.884689922480618 %\n"
          ]
        }
      ],
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour algorithm:\", valAccuracy*100, \"%\")\n",
        "\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier:\", valAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier. Now let us try another random split and check the validation accuracy. We will see that the validation accuracy changes with the split. This is because the validation set is small and the accuracy is highly dependent on the samples in the validation set. We can get a better estimate of the accuracy by using cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujm3cyYzEntE",
        "outputId": "4c168d1b-bbed-4218-c51c-4540995cb99a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour algorithm: 34.048257372654156 %\n"
          ]
        }
      ],
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(\n",
        "    alltraindata, alltrainlabel, 75 / 100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour algorithm:\", valAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNEZ5ToYBEDW",
        "outputId": "25f3015e-403e-4dae-e812-3f9d17541b5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 34.91795366795367 %\n"
          ]
        }
      ],
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "\n",
        "print(\"Test accuracy:\", testAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9zvdYY6evgI"
      },
      "source": [
        "> Exercise: Try to implement a 3 nearest neighbour classifier and compare the accuracy of the 1 nearest neighbour classifier and the 3 nearest neighbour classifier on the test dataset. You can use the KNeighborsClassifier class from the scikit-learn library to implement the K-Nearest Neighbors model. You can set the number of neighbors using the n_neighbors parameter. You can also use the accuracy_score function from the scikit-learn library to calculate the accuracy of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>cross-validation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute. You can reduce the number of splits to make it faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "outputs": [],
      "source": [
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "    \"\"\"\n",
        "    This function takes in the data, labels, split percentage, number of iterations and classifier function\n",
        "    and returns the average accuracy of the classifier\n",
        "\n",
        "    alldata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    alllabel: numpy array of shape (n,) where n is the number of samples\n",
        "    splitpercent: float which is the percentage of data to be used for training\n",
        "    iterations: int which is the number of iterations to run the classifier\n",
        "    classifier: function which is the classifier function to be used\n",
        "\n",
        "    returns: the average accuracy of the classifier\n",
        "    \"\"\"\n",
        "    accuracy = 0\n",
        "    for ii in range(iterations):\n",
        "        traindata, trainlabel, valdata, vallabel = split(\n",
        "            alldata, alllabel, splitpercent\n",
        "        )\n",
        "        valpred = classifier(traindata, trainlabel, valdata)\n",
        "        accuracy += Accuracy(vallabel, valpred)\n",
        "    return accuracy / iterations  # average of all accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3qtNar7Bbik",
        "outputId": "ff2bd12d-a0b3-43c8-ba0f-7862860de7d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy: 33.58463539517022 %\n",
            "Test accuracy: 34.91795366795367 %\n"
          ]
        }
      ],
      "source": [
        "avg_acc = AverageAccuracy(alltraindata, alltrainlabel, 75 / 100, 10, classifier=NN)\n",
        "print(\"Average validation accuracy:\", avg_acc*100, \"%\")\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "\n",
        "print(\"Test accuracy:\", Accuracy(testlabel, testpred)*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)Yes, **averaging the validation accuracy across multiple splits** (i.e., using techniques like **k-fold cross-validation**) can give more consistent and reliable results. Here’s why:\n",
        "\n",
        "### 1. **Reduced Variance**\n",
        "When you train and validate a model on a single split of the data, the performance can vary depending on how the data is split. If the split happens to include harder examples or imbalanced classes, the accuracy may fluctuate. By averaging the accuracy across multiple splits, you reduce the impact of any single split's characteristics, leading to a more **stable and representative performance** measure.\n",
        "\n",
        "### 2. **Better Generalization**\n",
        "Cross-validation ensures that the model is tested on different parts of the dataset, allowing you to see how well it generalizes to unseen data. If you only use one train-test split, the model might overfit that specific partition. Averaging across multiple splits mitigates this risk.\n",
        "\n",
        "### 3. **Insights into Model Robustness**\n",
        "When you average the validation accuracies across multiple splits, you also get a sense of how sensitive the model is to the data. If the accuracy is consistent across splits, it indicates the model is robust. If the accuracy varies a lot, it may indicate the model is more sensitive to the specific distribution of data in each split.\n",
        "\n",
        "### 4. **Cross-Validation Techniques**\n",
        "- **k-fold Cross-Validation**: The dataset is split into `k` equal parts, the model is trained on `k-1` parts, and the remaining part is used for validation. This process is repeated `k` times, with each fold used as the validation set once. The average accuracy across all `k` runs is reported.\n",
        "  \n",
        "    - **Example**: For 5-fold cross-validation, the dataset is split into 5 equal parts. The model is trained 5 times, with a different part used as the validation set each time. The final accuracy is the average of the 5 validation accuracies.\n",
        "\n",
        "- **Stratified k-fold**: If the dataset is imbalanced (e.g., classes have different numbers of samples), stratified k-fold ensures that each fold has a similar class distribution, improving consistency in the results.\n",
        "\n",
        "### 5. **How to Implement k-Fold Cross-Validation in PyTorch**\n",
        "Here's a simple way to implement k-fold cross-validation in PyTorch using `sklearn.model_selection.KFold`:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import KFold\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "# Define model, criterion, and optimizer\n",
        "model = SimpleNN()  # Use your own model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Load dataset\n",
        "dataset = MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# k-fold cross-validation\n",
        "k_folds = 5\n",
        "kf = KFold(n_splits=k_folds, shuffle=True)\n",
        "\n",
        "fold_accuracies = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
        "    print(f\"Fold {fold+1}/{k_folds}\")\n",
        "    \n",
        "    # Subset data for this fold\n",
        "    train_subset = Subset(dataset, train_idx)\n",
        "    val_subset = Subset(dataset, val_idx)\n",
        "    \n",
        "    # Loaders for training and validation\n",
        "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
        "    \n",
        "    # Train the model\n",
        "    for epoch in range(5):  # Train for 5 epochs as an example\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    \n",
        "    # Validate the model\n",
        "    correct, total = 0, 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    # Accuracy for this fold\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Validation Accuracy for fold {fold+1}: {accuracy:.2f}%\")\n",
        "    fold_accuracies.append(accuracy)\n",
        "\n",
        "# Average accuracy across all folds\n",
        "average_accuracy = sum(fold_accuracies) / k_folds\n",
        "print(f\"Average Validation Accuracy: {average_accuracy:.2f}%\")\n",
        "```\n",
        "\n",
        "### 6. **Conclusion**\n",
        "Averaging the validation accuracy across multiple splits using cross-validation techniques helps provide a **more consistent, reliable, and generalized performance estimate**. It helps to mitigate the risk of obtaining overly optimistic or pessimistic results based on a single, potentially non-representative train-test split.\n",
        "\n",
        "2)### 1. **Better Representation of the Dataset**\n",
        "In a single train-test split, the model is only trained on a subset of the data and validated/tested on another subset. This means the model might not encounter all the important patterns in the training data or the validation set might not fully represent the diversity of the overall dataset.\n",
        "\n",
        "- With **k-fold cross-validation**, each fold acts as both training and validation at some point. This ensures that the model gets to see all of the data in both roles (training and validation), providing a more holistic view of its performance.\n",
        "  \n",
        "### 2. **Reduction of Randomness**\n",
        "Single train-test splits are susceptible to random variations in the data. For instance, if the test set contains a higher concentration of outlier or difficult samples, it could lead to lower performance, or vice versa.\n",
        "\n",
        "- By **averaging across multiple splits**, cross-validation reduces the impact of random variations in the train-test split and gives a performance estimate that's less dependent on the particular division of the data.\n",
        "\n",
        "### 3. **Generalization**\n",
        "Since the model is tested on multiple different validation sets in cross-validation, it’s forced to generalize better across various data distributions, making the averaged accuracy closer to what you'd expect on truly unseen data (test set).\n",
        "\n",
        "- In contrast, a model trained and evaluated on a single split could potentially overfit the training set or not fully generalize well to the test set, leading to **overly optimistic or pessimistic estimates**.\n",
        "\n",
        "### 4. **Estimating Test Accuracy with Cross-Validation**\n",
        "When cross-validation gives you an average validation accuracy, that estimate is typically closer to what you would observe on a real, unseen test set because it accounts for more variability in the data. You can expect that:\n",
        "\n",
        "- The **cross-validated accuracy** will be closer to the performance on unseen data (test accuracy), as it simulates the process of training and testing on different partitions of the data multiple times.\n",
        "  \n",
        "- **Single train-test splits** may give you test accuracy estimates that could be overly optimistic or pessimistic based on the randomness of the split.\n",
        "\n",
        "### 5. **Potential Limitations**\n",
        "While cross-validation gives a more accurate estimate of test accuracy, it’s important to note that:\n",
        "- The final model deployed to the test set should be trained on the **entire training dataset**, not just on one of the k-folds.\n",
        "- There may still be slight differences between cross-validated accuracy and test accuracy because test sets are usually \"unseen\" and can contain outlier or different data distributions that the training/validation data did not.\n",
        "\n",
        "### Conclusion\n",
        "In most cases, **averaging the validation accuracy over multiple splits** via cross-validation provides a more reliable and **accurate estimate of test accuracy** than using a single train-test split. It helps to better simulate how the model would perform on unseen data by training and testing the model multiple times on different subsets of the data.\n",
        "\n",
        "3)Yes, increasing the number of iterations (or **splits/folds**) in techniques like **k-fold cross-validation** can provide a more accurate and reliable estimate of the test accuracy, but there are diminishing returns beyond a certain point. Let’s explore this in more detail:\n",
        "\n",
        "### 1. **Effect of Increasing the Number of Iterations (Folds)**\n",
        "\n",
        "- **Fewer Folds (e.g., 3-fold or 5-fold cross-validation)**:\n",
        "   - When using a lower number of folds (e.g., 3 or 5), each fold contains a relatively large portion of the dataset, meaning each model is trained on fewer data points and validated on more data points in each iteration.\n",
        "   - This might give a relatively good estimate of performance, but the model could still miss out on important patterns from not seeing enough diverse training examples in each fold.\n",
        "   - **Advantage**: Fewer iterations lead to faster training, but at the cost of slightly less accurate estimates.\n",
        "   \n",
        "- **More Folds (e.g., 10-fold or 20-fold cross-validation)**:\n",
        "   - As you increase the number of folds, the training set size in each iteration becomes larger, with smaller validation sets. This gives the model more data to train on in each fold and thus provides more robust and accurate results.\n",
        "   - Each sample is used more frequently in training, improving the model’s exposure to different parts of the data. In turn, this leads to better generalization and more stable estimates of performance.\n",
        "   - **Advantage**: More folds result in a more robust estimate of test accuracy because the model is trained on a larger variety of data in different combinations.\n",
        "\n",
        "### 2. **Better Estimates with Higher Iterations?**\n",
        "\n",
        "Generally, yes, **higher iterations (folds)** can lead to **better estimates** of test accuracy, but only up to a certain point:\n",
        "\n",
        "- **More reliable estimates**: Higher iterations reduce the variance in accuracy estimates across different validation sets, leading to a more reliable and accurate performance metric.\n",
        "  \n",
        "- **Reducing bias**: Training the model on a larger portion of the dataset in each iteration reduces bias (since the model is trained on more data), and smaller validation sets reduce the pessimism in accuracy estimates.\n",
        "\n",
        "However, the improvement in estimate accuracy **diminishes after a certain point**.\n",
        "\n",
        "### 3. **Diminishing Returns with Too Many Iterations**\n",
        "\n",
        "While higher iterations generally give better estimates, there’s a point of **diminishing returns**:\n",
        "\n",
        "- **10-fold cross-validation** is a common balance between reliable estimates and computational cost.\n",
        "- Increasing to **20-fold** or higher might provide only marginal improvements in estimate quality while significantly increasing computational time.\n",
        "  \n",
        "For example, if you go from 10-fold to 20-fold cross-validation:\n",
        "- The difference in accuracy estimates might only improve slightly, say from **0.5% to 1%**, but the computational cost would double.\n",
        "\n",
        "### 4. **Leave-One-Out Cross-Validation (LOO-CV)**\n",
        "The extreme case of increasing iterations is **Leave-One-Out Cross-Validation (LOO-CV)**, where each sample is used as a validation set, and all others are used for training. While this gives the most comprehensive use of data, it is:\n",
        "- **Very computationally expensive**, especially for large datasets.\n",
        "- **Highly variable**, as each validation set contains only a single data point, making the result more sensitive to outliers.\n",
        "\n",
        "In practice, this is not recommended unless you're working with a very small dataset.\n",
        "\n",
        "### 5. **Trade-Off Between Accuracy and Computation**\n",
        "Higher iterations (folds) do give better estimates, but they come at the cost of increased computation time:\n",
        "- **For small datasets**: It can be feasible to use more folds (e.g., 10-fold or even leave-one-out).\n",
        "- **For large datasets**: Using too many iterations might not be practical, and **5-10 folds** are usually sufficient.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "- **More iterations (folds)** generally lead to more accurate and consistent estimates of test accuracy, as they reduce the variance associated with the random train-test split.\n",
        "- However, after a certain point (e.g., beyond 10-fold cross-validation), the improvements in accuracy estimates become marginal, and the **computational cost increases significantly**.\n",
        "- Therefore, in most cases, using **5-10 folds** strikes a good balance between **accuracy** and **computational efficiency**.\n",
        "\n",
        "\n",
        "\n",
        "4)Yes, you can somewhat mitigate the limitations of a **very small train or validation dataset** by increasing the number of iterations (folds) in cross-validation. However, this approach has limitations and trade-offs. Let’s break down how increasing iterations helps and where its effectiveness diminishes:\n",
        "\n",
        "### 1. **How Increasing Iterations Helps with Small Datasets**\n",
        "\n",
        "- **Maximizing Data Usage**:\n",
        "   - In small datasets, a major concern is that the model doesn't have enough data to learn from. By increasing the number of iterations (or folds), you're effectively increasing the amount of data the model trains on in each fold. For example, in **10-fold cross-validation**, the model is trained on 90% of the data and validated on 10%. In contrast, a simple train-test split might only give the model 70-80% of the data for training.\n",
        "   - **More iterations mean more training data for each fold**, which helps the model generalize better by exposing it to a larger portion of the data.\n",
        "\n",
        "- **More Reliable Estimates**:\n",
        "   - With a small dataset, validation accuracy can vary widely depending on how the data is split. Increasing iterations reduces the variability in accuracy estimates by validating on multiple small subsets. This makes the performance estimate more stable and reliable, which is especially useful when dealing with limited data.\n",
        "  \n",
        "### 2. **The Limitation of Small Datasets**\n",
        "Despite increasing iterations, there are intrinsic limitations when dealing with very small datasets:\n",
        "\n",
        "- **Limited Information in the Data**:\n",
        "   - Small datasets inherently have less information. Increasing the number of folds doesn’t change the fact that the model is only being trained on a limited amount of data. While cross-validation helps maximize the use of available data, it **can’t generate new information**. So, even with more iterations, there’s only so much the model can learn.\n",
        "\n",
        "- **Model Complexity**:\n",
        "   - When working with very small datasets, a complex model may still overfit, even if you increase the number of folds. Cross-validation helps mitigate overfitting by validating on multiple sets, but with too little data, the model might still not generalize well.\n",
        "\n",
        "- **Diminishing Returns**:\n",
        "   - As mentioned earlier, increasing iterations provides diminishing returns after a certain point. For instance, moving from 5-fold to 10-fold cross-validation gives a noticeable benefit. However, moving from 10-fold to 20-fold or leave-one-out cross-validation (LOO-CV) might not significantly improve accuracy estimates while drastically increasing computation time.\n",
        "\n",
        "### 3. **Very Small Training Sets**\n",
        "- When the training set is extremely small (say 50 images per class), increasing the number of iterations might help you get a slightly better estimate of how well the model is performing. However:\n",
        "  - The model might still **struggle to generalize** because there isn’t enough diverse data for it to learn from.\n",
        "  - Even with higher iterations, the model could still **overfit**, particularly with deep learning models or highly complex architectures.\n",
        "\n",
        "### 4. **Very Small Validation Sets**\n",
        "- If you have a very small validation set, increasing the number of iterations can reduce the likelihood that the validation set is not representative of the overall dataset. With more iterations:\n",
        "  - The validation accuracy becomes more reliable because you’re validating on different subsets of the data across different folds.\n",
        "  - Each data point will eventually be part of the validation set in some fold, allowing for a more **comprehensive evaluation**.\n",
        "\n",
        "### 5. **Alternative Techniques for Small Datasets**\n",
        "\n",
        "When dealing with very small datasets, you can try other techniques beyond just increasing iterations to improve model performance and estimates:\n",
        "\n",
        "- **Data Augmentation**:\n",
        "   - Artificially increasing the size of the training set by augmenting existing data (e.g., rotating, flipping, cropping images) can help the model learn better and generalize more effectively. This is particularly helpful in image data.\n",
        "  \n",
        "- **Transfer Learning**:\n",
        "   - Pretrained models can be fine-tuned on small datasets. Since the pretrained model has already learned general features from a larger dataset, it can adapt well to your small dataset with fewer training examples.\n",
        "\n",
        "- **Simpler Models**:\n",
        "   - Use less complex models with fewer parameters. These models are less likely to overfit on small datasets.\n",
        "\n",
        "- **Regularization**:\n",
        "   - Applying techniques like **L2 regularization** or **dropout** can help prevent overfitting when training on small datasets.\n",
        "\n",
        "### 6. **Conclusion: Can Increasing Iterations Alone Solve the Problem?**\n",
        "While increasing the number of iterations can help improve the accuracy estimate and better utilize small datasets, it **cannot fully overcome the inherent limitations** of a small dataset:\n",
        "\n",
        "- It **maximizes data usage** and gives a more stable performance estimate by validating on different subsets of the data multiple times.\n",
        "- But if the dataset is very small, the model still faces challenges like **overfitting** and **lack of diversity** in the training data, which higher iterations alone cannot solve.\n",
        "\n",
        "Thus, increasing iterations is helpful but often needs to be combined with **data augmentation**, **simpler models**, or even **transfer learning** to achieve better performance on small datasets."
      ],
      "metadata": {
        "id": "5HoY4xKQ9_Sb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-SBxy1qevgJ"
      },
      "source": [
        "> Exercise: How does the accuracy of the 3 nearest neighbour classifier change with the number of splits? How is it affected by the split size? Compare the results with the 1 nearest neighbour classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To explore how the accuracy of a 3 nearest neighbor (KNN) classifier changes with the number of splits and how it is affected by split size, we can perform a series of experiments. Here's how to structure the experiments and analyze the results, followed by a comparison with a 1 nearest neighbor classifier.\n",
        "\n",
        "### Experiment Structure\n",
        "\n",
        "1. **Dataset Preparation**:\n",
        "   - Choose a dataset (e.g., the Iris dataset or MNIST).\n",
        "   - Split the dataset into different sizes (e.g., 60% training, 40% testing).\n",
        "\n",
        "2. **Define Number of Splits**:\n",
        "   - Use different numbers of splits (e.g., 5, 10, and 20-fold cross-validation).\n",
        "\n",
        "3. **KNN Classifier**:\n",
        "   - Implement both the 3-nearest neighbor (3-NN) classifier and the 1-nearest neighbor (1-NN) classifier.\n",
        "\n",
        "4. **Measure Accuracy**:\n",
        "   - For each number of splits and each classifier, calculate the average accuracy over multiple iterations.\n",
        "\n",
        "### Key Concepts to Consider\n",
        "\n",
        "1. **Effect of Number of Splits**:\n",
        "   - Increasing the number of splits typically leads to a more reliable estimate of the classifier's accuracy.\n",
        "   - With more splits, each training set contains a larger portion of the overall dataset, which can help the KNN classifier by providing more examples to learn from.\n",
        "\n",
        "2. **Effect of Split Size**:\n",
        "   - Smaller splits (more folds) can lead to more training instances per fold, potentially improving accuracy.\n",
        "   - However, smaller test sets may lead to higher variance in accuracy estimates.\n",
        "\n",
        "### Example Results (Hypothetical)\n",
        "\n",
        "1. **5-Fold Cross-Validation**:\n",
        "   - **3-NN Accuracy**: 85%\n",
        "   - **1-NN Accuracy**: 82%\n",
        "\n",
        "2. **10-Fold Cross-Validation**:\n",
        "   - **3-NN Accuracy**: 87%\n",
        "   - **1-NN Accuracy**: 84%\n",
        "\n",
        "3. **20-Fold Cross-Validation**:\n",
        "   - **3-NN Accuracy**: 88%\n",
        "   - **1-NN Accuracy**: 85%\n",
        "\n",
        "### Analysis\n",
        "\n",
        "1. **Accuracy Change with Splits**:\n",
        "   - The accuracy of both classifiers generally increases with the number of splits.\n",
        "   - The 3-NN classifier tends to perform better than the 1-NN classifier, as it reduces the impact of noise and outliers that might mislead the decision in 1-NN.\n",
        "\n",
        "2. **Impact of Split Size**:\n",
        "   - As the number of splits increases (for instance, from 5 to 20), the accuracy of the 3-NN classifier consistently improves. This is likely because:\n",
        "     - Each fold has a sufficient amount of training data for the classifier to learn the underlying patterns.\n",
        "     - The model benefits from the majority vote mechanism of 3-NN, which averages out noise better than 1-NN.\n",
        "   - Conversely, with fewer splits, the test sets are larger, which might lead to more variability in the estimates.\n",
        "\n",
        "3. **Comparison with 1-NN**:\n",
        "   - The 1-NN classifier, while simple and often effective, tends to be more sensitive to noise. This sensitivity might lead to slightly lower accuracy compared to the 3-NN classifier across different splits.\n",
        "   - The 3-NN classifier benefits from considering multiple neighbors, which helps in cases where the nearest neighbor might be an outlier.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "- **Summary**:\n",
        "  - Increasing the number of splits generally leads to improved accuracy for both 1-NN and 3-NN classifiers, but 3-NN consistently outperforms 1-NN due to its robustness against noise.\n",
        "  - Split size affects accuracy as well; smaller splits tend to yield more reliable estimates but can lead to variance if test sizes are too small.\n",
        "\n",
        "- **Practical Implications**:\n",
        "  - When using KNN classifiers, especially in cases with potential noise or outliers, opting for a multi-neighbor approach (like 3-NN) is generally advisable.\n",
        "  - The choice of the number of splits should balance between computational efficiency and the need for reliable accuracy estimates.\n",
        "\n",
        "### Next Steps\n",
        "To solidify these observations, consider running the actual experiments on a chosen dataset, plotting the accuracy against the number of splits, and analyzing the resulting patterns to draw further conclusions."
      ],
      "metadata": {
        "id": "MvqIaX5nAvfD"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}